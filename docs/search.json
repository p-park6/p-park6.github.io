[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a recent Graduate student at the Bren School of Environmental Science and Management, and have finished up their data science program. I have learned a whole arsenal of tools as a novice data scientist such as geospatial analysis, machine learning, and data visualization.\nDuring my time here at Bren, I have completed a capstone project, which focuses on easier access to Climate Dataset models. My team and I used these climate datasets to help visualize possible extreme predictions for the Channel Island sanctuary in a web application format.\nIn my spare time, I love to bake and write letters! I love to try out new cookie recipes (which also gives me an excuse to spoil my sweet tooth). I also love writing letters to friends who are not in the same city as me! I’ve been more picky about the pens that I use to write these letters, so shoot me a message if you have any pen recommendations!"
  },
  {
    "objectID": "blog/2023-12-12-aqi-thomas-fire/index.html",
    "href": "blog/2023-12-12-aqi-thomas-fire/index.html",
    "title": "Analysis of the Air Quality caused by the Thomas Fire",
    "section": "",
    "text": "Link to associated github is here. More information such as analysis and notebook can be found here."
  },
  {
    "objectID": "blog/2023-12-12-aqi-thomas-fire/index.html#datasets-used-in-this-analysis",
    "href": "blog/2023-12-12-aqi-thomas-fire/index.html#datasets-used-in-this-analysis",
    "title": "Analysis of the Air Quality caused by the Thomas Fire",
    "section": "Datasets used in this analysis:",
    "text": "Datasets used in this analysis:\n\nAir quality data\n\n\nAir Quality Data for 2017\nAir Quality Data for 2018\nInformation on AQI here\n\n\n[Landsat Collection 2 Surface](Reflectance(https://www.usgs.gov/landsat-missions/landsat-collection-2-surface-reflectance)\n\n\nAdditional information: -Landsat Satellite homepage"
  },
  {
    "objectID": "blog/2023-12-12-aqi-thomas-fire/index.html#aqi-analysis",
    "href": "blog/2023-12-12-aqi-thomas-fire/index.html#aqi-analysis",
    "title": "Analysis of the Air Quality caused by the Thomas Fire",
    "section": "AQI Analysis",
    "text": "AQI Analysis\n\nLoad Necessary Packages\n\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport geopandas as gpd\nimport xarray as xr\nimport rioxarray as rioxr\nimport matplotlib.patches as mpatches\nfrom matplotlib.markers import MarkerStyle\n\nfrom shapely import Point\nfrom shapely import Polygon\nfrom shapely import box\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\n\n\n\nImport AQI Data\nAQI data was downloaded from the EPA website. Both 2017 and 2018 were download to see the air quality for both years. I’ll first go ahead and read in the data and see what the two datasets look like.\n\n# read in daily AQI 2017 zip file from url\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\n\n# read in daily AQI 2018 zip file from url\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n\nprint(aqi_17.head())\nprint(aqi_18.head())\n\n\n\nAQI Data Wrangling\nNow that I have a sense of what is contained in these datasets, I want to simplify them into one dataset. However, the data provided gives us information for the whole country. Since we are focusing on Santa Barbara County AQI, I’ll go ahead and join them into one dataset using the concat function as well as subset for Santa Barbara county. Then I’ll go ahead and filter for columns that only include the information I am interested in. I’ll also go ahead and make sure that the date column is in datetime to make working with dates easier.\n\n#join the two datasets together\naqi = pd.concat([aqi_17, aqi_18])\n\n# change column names to lowercase and replace spaces with '_'\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')\n\n#filter for observations that are in Santa Barbara County\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n#remove not needed columns from the data frame\naqi_sb = aqi_sb.drop(columns=['state_name','county_name','state_code','county_code'])\n\n# change date type to be in datetime object (can be ran only one time)\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n#change index to be set to date\naqi_sb = aqi_sb.set_index('date')\n\n\n\nAQI data visualization\nNow we have a filtered dataset, I’ll go ahead and use that to make a graph, visualizing the air quality over the two years. I’ll also go ahead and calculate a 5 day rolling average to see if the air quality during that time was significant or not.\n\n#create a new column that is the 5 day rolling average mean\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n#Plot data\naqi_sb.plot(y = ['aqi', 'five_day_average'], #plot both aqi and five_day_average column\n           title = 'AQI in Santa Barbara', #add title to graph\n           xlabel = 'Year', #add x label to graph\n           ylabel = 'Air Quality Index (AQI)', #add y label to graph\n           color = {'aqi': 'blue', #add blue to aqi line\n                    'five_day_average': 'orange' #add orange to five_day_average line\n                   },\n           )\n\n\nTakeaways: At around December 2017, There is a large spike in the AQI with both the daily and 5 day average. Surprisingly, there in an increase in AQI at January 2017. I assume there was a fire at the beginning of that year. However, looking at the 5 day rolling average, it seemed that the fire in 2018 (Thomas Fire) has a significant impact towards the AQI compared to last year’s 5 day rolling average."
  },
  {
    "objectID": "blog/2023-12-12-aqi-thomas-fire/index.html#thomas-fire-area-burn-analysis",
    "href": "blog/2023-12-12-aqi-thomas-fire/index.html#thomas-fire-area-burn-analysis",
    "title": "Analysis of the Air Quality caused by the Thomas Fire",
    "section": "Thomas Fire area burn Analysis",
    "text": "Thomas Fire area burn Analysis\nSame as before, let’s go ahead and load in our data needed for this analysis. For this step, I’ll also go ahead and do everything in one code chuck as it follows a similar pattern as seen above. You’ll see that I did a step pertaining to squeezing and dropping a band. That is because this is a raster dataset. If the band is not dropped, you will not be able to map it.\n\n# Read in bands dataset\n#create pathway\nca_bands_fp = os.path.join(os.getcwd(),'data','landsat8-2018-01-26-sb-simplified.nc')\n#read data using pathway\nca_fires_bands_2017 = rioxr.open_rasterio(ca_bands_fp)\n\n#get rid of band in ca_fires_band_2017\n# original dimensions and coordinates\nprint('original dimensions: ', ca_fires_bands_2017.dims,'\\n')\n\n# remove length 1 dimension (band)\nca_fires_bands_2017 = ca_fires_bands_2017.squeeze()\nprint('removed band from dimensions: ', ca_fires_bands_2017.dims,'\\n')\n\n# remove coordinates associated to band\nca_fires_bands_2017 = ca_fires_bands_2017.drop('band')\nprint('new dimensions: ', ca_fires_bands_2017.dims)\n\n                     \n#change column names:\n#make columns lower case\nca_fires_perimeter_2017.columns = ca_fires_perimeter_2017.columns.str.lower()\n\n#change crs to the bands raster data\nca_fires_perimeter_2017 = ca_fires_perimeter_2017.to_crs(ca_fires_bands_2017.rio.crs)\n\n#create thomas fire perimeter\nthomas_perimeter = ca_fires_perimeter_2017[ca_fires_perimeter_2017['fire_name']=='THOMAS']\n\n\n#create map\n#plot an empty plot\nfig, ax = plt.subplots(figsize = (8,6))\n\n\n#plot CA fire bands map as the base\nca_fires_bands_2017[['swir22', #subset for the three wanted bands\n                     'nir08', \n                     'red']].to_array().plot.imshow(ax = ax,\n                                                    robust = True)\n\n#plot thomas fire perimeter over base map\nthomas_perimeter.plot(ax = ax, edgecolor = \"red\", color = 'none') #plot thomas_perimeter\nthomas_perimeter_patches = mpatches.Patch( color = \"red\", #create label for legend and change color\n                                    label = \"Thomas Fire Perimeter\") #set label\n\n#set title for map\nax.set_title('Thomas Fire Perimeter in 2017',\nfontsize=20) #change font size\n\n#add legend onto map\nax.legend(handles = [thomas_perimeter_patches], frameon=False, loc='upper left', bbox_to_anchor = (1, 1))\n\n\nTakeaways: Here we can see the amount of area that was burned by the Thomas fire. We can notice that there were some burnt area on the left, but none nearly as big as the Thomas Fire burn. This large area of burn would almost make sense why a mudslide followed in its tracks. The open space caused by the fire made it more possible for the surface soil to be washed away as there were no plants to keep the soil intacted."
  },
  {
    "objectID": "blog/2023-12-15-la-holc-grade-inequality/index.html",
    "href": "blog/2023-12-15-la-holc-grade-inequality/index.html",
    "title": "Los Angeles County’s history with HOLC grade inequality",
    "section": "",
    "text": "During the 1930’s the Home Owners’ Loan Corporation (HOLC), as part of the New Deal, rated neighborhoods based on their perceived safety for real estate investment. Their ranking system, (A (green), B (blue), C (yellow), D (red)) was then used to block access to loans for home ownership. While it was used to promote financial stimulation, it consequentially has been used for racial segregation, a history that is still visible in the United states. Informally known as “redlining”, this practice has had widely-documented consequences not only for community wealth, but also health.(Gee 2008) Redlined neighborhoods have less greenery(Nardone et al. 2021) and are hotter than other neighborhoods.(Hoffman, Shandas, and Pendleton 2020)\nThe National Community Reinvestment Coalition (NCRC) released a document highlighting what the redlining community is, how it has affected certain communities, and the potential impact gentrification has on these communities. 1 This study was able to take a deep dive into how this segregation impacts minority groups and the lack of investment in these communities to bring positive change.\nThis blog will cover a number of analysis including:\n\nVarious environmental factors that can impact a community’s wellbeing\nHow many census block group sites are in these redlining areas\nImpact on recreational activities based on HOLC grade groups\n\n\n\n\n\nWe will be working with data from the United States Environmental Protection Agency’s EJScreen: Environmental Justice Screening and Mapping Tool.\nAccording to the US EPA website:\n“This screening tool and data may be of interest to community residents or other stakeholders as they search for environmental or demographic information. It can also support a wide range of research and policy goals. The public has used EJScreen in many different locations and in many different ways.\nEPA is sharing EJScreen with the public: - to be more transparent about how we consider environmental justice in our work, - to assist our stakeholders in making informed decisions about pursuing environmental justice and, - to create a common starting point between the agency and the public when looking at issues related to environmental justice.”\nEJScreen provides on environmental and demographic information for the US at the Census tract and block group levels. This blog will be working with block group data that has been downloaded from the EPA site.\n\n\n\nA team of researchers, led by the Digital Scholarship Lab at the University of Richmond have digitized maps and information from the HOLC as part of the Mapping Inequality project.\nI will be working with maps of HOLC grade designations for Los Angeles. Information on the data can be found here.2\n\n\n\nThe Global Biodiversity Information Facility is the largest aggregator of biodiversity observations in the world. Observations typically include a location and date that a species was observed. I will be working observations of birds from 2021 onward."
  },
  {
    "objectID": "blog/2023-12-15-la-holc-grade-inequality/index.html#datasets",
    "href": "blog/2023-12-15-la-holc-grade-inequality/index.html#datasets",
    "title": "Los Angeles County’s history with HOLC grade inequality",
    "section": "",
    "text": "We will be working with data from the United States Environmental Protection Agency’s EJScreen: Environmental Justice Screening and Mapping Tool.\nAccording to the US EPA website:\n“This screening tool and data may be of interest to community residents or other stakeholders as they search for environmental or demographic information. It can also support a wide range of research and policy goals. The public has used EJScreen in many different locations and in many different ways.\nEPA is sharing EJScreen with the public: - to be more transparent about how we consider environmental justice in our work, - to assist our stakeholders in making informed decisions about pursuing environmental justice and, - to create a common starting point between the agency and the public when looking at issues related to environmental justice.”\nEJScreen provides on environmental and demographic information for the US at the Census tract and block group levels. This blog will be working with block group data that has been downloaded from the EPA site.\n\n\n\nA team of researchers, led by the Digital Scholarship Lab at the University of Richmond have digitized maps and information from the HOLC as part of the Mapping Inequality project.\nI will be working with maps of HOLC grade designations for Los Angeles. Information on the data can be found here.2\n\n\n\nThe Global Biodiversity Information Facility is the largest aggregator of biodiversity observations in the world. Observations typically include a location and date that a species was observed. I will be working observations of birds from 2021 onward."
  },
  {
    "objectID": "blog/2023-12-15-la-holc-grade-inequality/index.html#footnotes",
    "href": "blog/2023-12-15-la-holc-grade-inequality/index.html#footnotes",
    "title": "Los Angeles County’s history with HOLC grade inequality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMichell, B., & Franco, J. (2018, March 20). HOLC “redlining” maps: The persistent structure of segregation and economic inequality. https://ncrc.org/holc/ ↩︎↩︎\nRobert K. Nelson, LaDale Winling, Richard Marciano, Nathan Connolly, et al., “Mapping Inequality,” American Panorama, ed. Robert K. Nelson and Edward L. Ayers, accessed October 17, 2023, https://dsl.richmond.edu/panorama/redlining/↩︎"
  },
  {
    "objectID": "blog/2023-12-15-nitrogen-analysis/index.html",
    "href": "blog/2023-12-15-nitrogen-analysis/index.html",
    "title": "Statistically Analysing Nitrogen concentration in Vernal Pools",
    "section": "",
    "text": "Overview\nNutrients are the essential bases for plant to be able to grow. Nitrogen, phosphorus, and potassium are a few of the necessary nutrients that plants need in order to survive. The issue is when too much of these nutrients are inputted into the environment. This threatens the ecosystem such as trophic levels, increase of invasive species, and increase disease (Kneitel and Lessin 2010). One particular ecosystem, vernal pools are more sensitive to these changes. Vernal pools are seasonal wetlands, meaning they are full of water during the winter season and are empty and dry during the summer season. About 90% of wetlands have been lost from urban planning and the expansion of infrastructure in these location.\nOne of the issues with urban expansion is the changes in natural drainage in locations that include vernal pools in these areas. Runoff may be increasing or decreasing the amount of water that end up in these pools. One other concern is that vernal pools that are close to urban location may have an increase concentration of nutrients. One of the reasoning is that it is collecting the nutrients from nearby yards with a constant application of an influx of nutrients.\nBefore, I had done a small project concerning the nitrogen concentration found in plant and soil matter. My hypothesis looked at vernal pool’s proximity to urban spaces and the amount of nitrate as well as ammonium concentrations in these pools and how that affected the amount of nonnative plant percent cover in these areas. I had theorized that the vernal pools that were close or even had a direct pipe of runoff into these pools had an increase amount of nitrate and ammonium concentrations in it. These were the relationships as well as the hypothesis that I came up with: - Overall, the relationship between nonnative percent cover and nutrients were as followed:\n\\[\\text{Nonnative Percent Cover} = \\beta_0 + \\beta_1 \\text{nutrients} + \\epsilon\\]\n\nTo refine this equation to my project’s goals, I rearranged the equation to include nitrate and ammonium concentration depending on if they came from plant matter or soil matter: \\[\\text{Nonnative Percent Cover} = \\beta_0 + \\beta_1 \\text{nitrate and ammonium conc. soil} + \\epsilon\\] as well as \\[\\text{Nonnative Percent Cover} = \\beta_0 + \\beta_1 \\text{nitrogen conc. plant} + \\epsilon\\]\nMy hypothesis testing included the general syntax that I used when statistically analyzing my data:\n\n\\[H_{0}: \\mu_{Nonnative plant cover} - \\mu_{Native plant cover} = 0\\]\n\\[H_{A}: \\mu_{Nonnative plant cover} - \\mu_{Native plant cover} \\neq 0\\]\nIn this project, I was not able to find a definitive relationship between the nitrogen concentration and nonnative plant cover in my original analysis.\nHowever, for this blog, I am going to take a step back and see if there are any interesting trends inside my data. For this analysis, I am going to look if there is a relationship between the location of where the samples were located and if that impacts the amount of nitrogen concentration at those areas. Since I am taking a step back for this analysis, I’ll be looking at nutrients relating to nitrogen. In this case, I’ll be looking at nitrogen concentrations for plant matter and nitrate concentrations for soil matter.\nAs mentioned above, vernal pools are a sensitive ecosystem. Unfortunately, as there are only such few number left from its original amount, limited research has been done on these ecosystems. Few research papers have been published that cover the nutrient concentration in vernal pools and instead cover wetlands as a whole. One paper, “Litter breakdown among intermittently connected and unconnected geographically isolated Wetlands: how nutrient inputs alter wetland function”(Smith et al. 2022) talks about the impact increased nutrients can have on the breakdown on litter. This brings up a certain questions: does litter decomposition happen at a more rapid pace depending on if the litter is at the edge of the pool or at the center of the pool?\n\n\nData Visualization\nBefore doing some data visualizations, I’ll go ahead and read in the libraries I need in order to properly do my analysis.\n\n#import libraries\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(gstat)\nlibrary(sf)\nlibrary(automap)\nlibrary(patchwork)\nlibrary(viridis)\nlibrary(broom)\nlibrary(stargazer)\nlibrary(sjPlot)\n\nLet’s go ahead and load in the datasets that we will be using.\n\nnorth_parcel &lt;- read_csv(\"data/np_pc.csv\")\nnitrogen_soil &lt;- read_csv(\"data/Nitrogen_soil_5.csv\")\nnitrogen_plant &lt;- read_csv(\"data/Nitrogen_plant (5).csv\")\n\nBefore doing some data visualization, I need to clean up the columns and combine them to get a simplified dataset to work with. I’ve simplified the columns and combined them with another dataset that contained the coordinates of where I got my samples.\n\n#join soil dataset and north parcel dataset together\n#tidy up north_parcel dataset to have information that is relevent\nnorth_parcel_clean &lt;- north_parcel %&gt;% \n  dplyr::select(`ObjectID`, \n         `Vernal Pool Name or Number`, \n         `Vernal Pool Zone`, \n         `Transect Distance of Quadrat`, \n         x, \n         y) %&gt;% \n  rename(Pool = `Vernal Pool Name or Number`, \n         Zone = `Vernal Pool Zone`,\n         Distance = `Transect Distance of Quadrat`)\n\n#replace the NA value with edge to complete the dataframe\nnorth_parcel_clean$Zone[43] &lt;- \"edge\"\n\n\n#tidy up nitrogen_soil dataset\nnitrogen_soil_clean &lt;- nitrogen_soil %&gt;% \n  mutate(Zone = ifelse(Zone == \"E\", \"edge\", Zone),\n         Zone = ifelse(Zone == \"T\", \"transition\", Zone),\n         Zone = ifelse(Zone == \"M\", \"bottom\", Zone)) %&gt;% \n  select(-ObjectID)\n#  rename(nitrate_ug = `Nitrate (ug/g*dm)`)\n\n\n#join north_parcel_clean with soil dataset\nsoil_north_join &lt;- inner_join(nitrogen_soil_clean, north_parcel_clean, by = c('Pool', 'Zone', 'Distance'))\n\n#join nitrogen plant and north parcel dataset together\n#tidy up nitrogen_soil dataset\nnitrogen_plant_clean &lt;- nitrogen_plant %&gt;% \n  mutate(Zone = ifelse(Zone == \"E\", \"edge\", Zone),\n         Zone = ifelse(Zone == \"T\", \"transition\", Zone),\n         Zone = ifelse(Zone == \"M\", \"bottom\", Zone)) %&gt;% \n  rename(nitrogen_ug = nitrogen)\n\n#rename incorrect item names to correct name in nitrogen_plant dataset\nnitrogen_plant_clean$Pool[32:36] &lt;- \"PH1\"\n\n#join north_parcel_clean with soil dataset\nplant_north_join &lt;- inner_join(nitrogen_plant_clean, north_parcel_clean, by = c('Pool', 'Zone', 'Distance'))\n\nNow that I have two datasets including data that I am interested in, I’ll go ahead and do some visualization to what types of statistical tests I want to run. I’ll first plot the two graphs, each relating to soil and plant and see what types of trends appear in these graphs.\n\n#for soil graph\nggplot(data = soil_north_join, aes(y = `Nitrate (ug/g*dm)`, x = Zone)) + \n  geom_boxplot() + \n  labs(y = \"Nitrate (ug/g*dm)\",\n       x = \"Zone\")\n\n\n\n#for plant graph\nggplot(data = plant_north_join, aes(y = nitrogen_ug, x = Zone)) + \n  geom_boxplot() + \n  labs(y = \"Nitrogen (ug)\",\n       x = \"Zone\")\n\n\n\n\nI’m also curious about the normality of the data. I’ll go ahead and plot a histogram to see if the data is normally distributed for the soil data first.\n\n#check for normality by plotting histogram to see distribution:\n#soil\nhist(soil_north_join$`Nitrate (ug/g*dm)`)\n\n\n\n\nThe histogram for the soil samples looks as if the data is not normally distributed. It looks to be right tailed skewed. I’ll have to transform the data to see if I can get a more accurate normally distributed histogram.\n\n#plot histogram of transformed data\n#log data\nhist(log(soil_north_join$`Nitrate (ug/g*dm)`))\n\n\n\n#squared data\nhist(sqrt(soil_north_join$`Nitrate (ug/g*dm)`))\n\n\n\n#add another column for log transformed data on dataset\nsoil_north_join$log_nitrate &lt;- log(soil_north_join$`Nitrate (ug/g*dm)`)\n\nViewing the histograms of my transformed data, it seems that the log transformed data is distributed most normally. I’ll go ahead and use the log transformed data to do my analysis.\nLet’s also go ahead and view the distribution for the plant data. I’ll also plot a histogram to see how the data is distributed. I’ll also transform the data in the same cell to see how it changes the distribution.\n\n#check for normality by plotting histogram to see distribution:\n#plant\nhist(plant_north_join$nitrogen_ug)\n\n\n\n#log data\nhist(log(plant_north_join$nitrogen_ug))\n\n\n\n#squared data\nhist(sqrt(plant_north_join$nitrogen_ug))\n\n\n\n#add another column for squared transformed data on dataset\nplant_north_join$sqrt_nitrogen &lt;- sqrt(plant_north_join$nitrogen_ug)\n\nIt is hard to tell which data looks normally distributed between the three different types of histogram. For this analysis, I’ll go ahead and use the squared transformed data as that seems to be closest to a normally distributed histogram.\n\n\nAnalysis plan\nFrom my preliminary analysis of the data, I have decided to do three types of statistical analysis:\n\nLinear regression Analysis\n\n\nFrom the analysis of the plots, it looks like there is some sort of pattern in terms of the plant data. I would like to see if there is any difference between the 3 sites and see how much the sites can explain the nitrogen concentration found in vernal pools as a whole.\n\n2, Hypothesis testing\n\nLooking at the histogram results, it would be interesting to see how significant my results are and if there is conclusive evidence about if different parts of a vernal pool houses a certain concentration amount of nitrogen.\n\n\nSpatial statistics\n\n\nAs I have geographical points for this data, it would be interesting to see the nitrogen concentrations throughout the whole field site. I would be using Kriging in order to predict the nitrogen levels and use that data to plot on a map to visualize the nitrogen concentrations throughout the field site.\n\n\nLimitations:\nA main limitation of this study is the small sample size. In total, there was around 80 samples that I was able to use my analysis on. As seen later in this blog, almost all my results were outputted as insignificant. If this analysis were to be done again, it would be nice to look at a big amount of samples to get a more accurate picture of the field and the analysis that followed.\n\n\n\nStatistical summary\n\nLinear regression Analysis\n\nI’ll first run my soil linear regression to see how different the zones are from one another.\n\n\n\n\n\n \nlog nitrate\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n1.43\n1.06 – 1.79\n&lt;0.001\n\n\nZone [edge]\n-0.36\n-0.87 – 0.16\n0.171\n\n\nZone [transition]\n-0.10\n-0.62 – 0.42\n0.693\n\n\nObservations\n36\n\n\nR2 / R2 adjusted\n0.059 / 0.002\n\n\n\n\n\n\n\nFrom our summary table, we can see that there may be a difference in nitrogen concentrations depending on the zone from looking at the estimate coefficients. However, when looking at the R2 adjusted, our number that we get is 0.011. This is a very small number which we can interpret as amount of nitrogen concentrations we find can be explained by the location in a pool.\nWe’ll go ahead and look at our plant regression and see what summary we get\n\n#plant linear regression analysis\nlinreg_plant &lt;- lm(sqrt_nitrogen ~ Zone, data = plant_north_join)\ntab_model(linreg_plant, CSS =list(css.depvarhead = '+color: red;',\n                                 css_theme(\"cells\")))\n\n\n\n\n \nsqrt nitrogen\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n0.10\n0.09 – 0.11\n&lt;0.001\n\n\nZone [edge]\n-0.00\n-0.01 – 0.01\n0.866\n\n\nZone [transition]\n-0.01\n-0.02 – 0.01\n0.293\n\n\nObservations\n36\n\n\nR2 / R2 adjusted\n0.038 / -0.020\n\n\n\n\n\n\n\nHere we can see that there is basically no difference in the nitrogen concentration depending on where you sample the plants. This would make sense as plants would absorb the same amount of nirogen depending on how much there was in the system in the first place.\nLooking at these two analysis, I can conclude that location in a pool may not hold a threshold for a specific amount of nitrogen. In the next test, I’ll go ahead and still challenge that belief by testing my hypothesis\n\nHypothesis testing\n\nFrom what I am looking at, I’ll go ahead and test the following hypothesis:\n\\[H_{0}: \\mu_{Edge conc.} = \\mu_{Transition conc.} = \\mu_{Middle conc.}\\]\n\\[H_{A}: \\mu_{Edge conc.} \\neq \\mu_{Transition conc.} \\neq \\mu_{Middle conc.}\\]\nIn order to test my hypothesis, I’ll have to conduct an ANOVA test. An ANOVA test is done when you have 3 or more variable you are testing to one another to see if they are the same or different, as seen in my hypothesis above. For this, I’ll be conducting a one-way ANOVA test and see what p-values I get. Above, I have\n\n#soil anova\nnitrogen_soil_aov &lt;- aov(log_nitrate ~ Zone, data = soil_north_join)\ntab_model(nitrogen_soil_aov, CSS =list(css.depvarhead = '+color: red;',\n                                 css_theme(\"cells\")))\n\n\n\n\n \nlog nitrate\n\n\nPredictors\np\n\n\nZone\n0.365\n\n\nResiduals\n\n\n\nObservations\n36\n\n\nR2 / R2 adjusted\n0.059 / 0.002\n\n\n\n\n\n\n#plant anova\nnitrogen_plant_aov &lt;- aov(sqrt_nitrogen ~ Zone, data = plant_north_join)\ntab_model(nitrogen_plant_aov, CSS =list(css.depvarhead = '+color: red;',\n                                 css_theme(\"cells\")))\n\n\n\n\n \nsqrt nitrogen\n\n\nPredictors\np\n\n\nZone\n0.524\n\n\nResiduals\n\n\n\nObservations\n36\n\n\nR2 / R2 adjusted\n0.038 / -0.020\n\n\n\n\n\n\n\nLooking at the p-values, it is clear there is no significance between the zones and nitrogen concentrations as the p-values from both tests is much higher than 0.05. From this information, I conclude that I fail to reject the null hypothesis.\n\nSpatial Analysis\n\nWhile the first two results were clear signs that location does not have an impact on nitrogen concentration, I am still curious about the predicted locations of levels on nitrogen concentration throughout the study site. I’ll go ahead and try kriging my results to see what the model predicts.\n\n#convert dataset coordinates to sf\nplant_conc_sf &lt;- st_as_sf(plant_north_join, coords = c(\"x\", \"y\"), crs = 3310) %&gt;% #crs is for california\n  cbind(st_coordinates(.))\n\n#create an auto fit variogram\nv_mod_full &lt;- automap::autofitVariogram(sqrt_nitrogen~1, plant_conc_sf)\n\n#plot variogram\nplot(v_mod_full)\n\n\n\n\n\n#convert dataset coordinates to sf\nsoil_conc_sf &lt;- st_as_sf(soil_north_join, coords = c(\"x\", \"y\"), crs = 3310) %&gt;% #crs is for california\n  cbind(st_coordinates(.))\n\n#create an auto fit variogram\nv_mod_full &lt;- automap::autofitVariogram(sqrt_nitrogen~1, plant_conc_sf)\n\n#plot variogram\nplot(v_mod_full)\n\n\n\n\nLooking only at the two variograms for both soil and plant, I can conclude that kriging, if done, will not be accurate. This is because for both graphs, the range and the variance are very small numbers. Even if I was able to make a Krigine model, I would not be confident in the prediction model that was given to me.\n\n\nNext steps\nIf I was able to improve on this study, I would have gotten many more data points. A sample size of 80 is much too small and leaves room for lots of error with outliers completely skewing the analysis. A bigger sample size would make me feel more confident in my results that I obtain. It would also overpower any outliers there are in the data as well.\n\nSide notes\nAll analysis can be found here as well as the data used.\nData Access note: All the data used here in this analysis is data that I have collected. Data can be publicly accessed through my github repository linked above. If used, please cite properly.\n\n\n\n\n\n\nReferences\n\nKneitel, Jamie M., and Carrie L. Lessin. 2010. “Ecosystem-Phase Interactions: Aquatic Eutrophication Decreases Terrestrial Plant Diversity in California Vernal Pools.” Oecologia 163 (2): 461–69. https://doi.org/10.1007/s00442-009-1529-0.\n\n\nSmith, Chelsea R., Stephen W. Golladay, Carla L. Atkinson, and Brian A. Clayton. 2022. “Litter Breakdown Among Intermittently Connected and Unconnected Geographically Isolated Wetlands: How Nutrient Inputs Alter Wetland Function.” Wetlands 42 (6): 57. https://doi.org/10.1007/s13157-022-01567-1.\n\nCitationBibTeX citation:@online{park2023,\n  author = {Park, Patty},\n  title = {Statistically {Analysing} {Nitrogen} Concentration in\n    {Vernal} {Pools}},\n  date = {2023-12-15},\n  url = {https://p-park6.github.io/blog/2023-12-15-nitrogen-analysis/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPark, Patty. 2023. “Statistically Analysing Nitrogen Concentration\nin Vernal Pools.” December 15, 2023. https://p-park6.github.io/blog/2023-12-15-nitrogen-analysis/."
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html",
    "href": "blog/2024-03-09-young-adult-migration/index.html",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "",
    "text": "The intention of this blog post is to show the steps I took to create my final infographic piece. This blog will detail the steps I took to create each infographic piece. I’ll also detail how I did my preliminary analysis of the data to help shape the questions I asked. This dataset, found on migrationpatterns.org, details where young adults are from and where they choose to move to for their adult portion of their lives. This blog also serves to integrate key components of what makes an inforgraphic strong in each visualization. I also hopes that this helps inspire you to create your own infographic as well!"
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#data-exploration-or-whats-important-about-this-dataset",
    "href": "blog/2024-03-09-young-adult-migration/index.html#data-exploration-or-whats-important-about-this-dataset",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Data Exploration (or what’s important about this dataset?)",
    "text": "Data Exploration (or what’s important about this dataset?)\nBefore starting this project, I did a good amount of data exploration to understand what the data meant and what each column was representing. This dataset came with a metadata sheet that helped me understand what each column heading meant. However, the metadata was written in such a complex manner that it took me a couple of weeks to understand what it all meant.\n\nShown here is a snippit of what the metadata looks like. After a couple of readings through the metadata and doing my own research, I realized what each column header stood for in the datasets and how to use each column to accurately show the numbers."
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#data-wrangling-basically-shaping-the-data",
    "href": "blog/2024-03-09-young-adult-migration/index.html#data-wrangling-basically-shaping-the-data",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Data Wrangling (basically shaping the data)",
    "text": "Data Wrangling (basically shaping the data)\nNot much wrangling was done for this project as most of the data process was done by the researchers end. I did, however, had to add other datasets to get the information I needed in order to create my perfect infographic. These are the steps I took to shape the data:\n\n#load in libraries\nlibrary(tidyverse)\nlibrary(states)\nlibrary(maps)\nlibrary(patchwork)\nlibrary(scales)\nlibrary(showtext)\n\n\n#load in data\nod_pooled &lt;- read_csv(\"data/od_pooled.csv\") #dataset technically not 'separated' by a specific factor\n\n# Most popular cities not separated by a certain factor\ntop_ten_pooled &lt;- od_pooled %&gt;% #using od_pooled\n  select(o_cz_name, o_state_name, d_cz_name, d_state_name, n, n_tot_o, pool) %&gt;%#select columns we are interested in, helps get rid of repeats\n  filter(d_cz_name == \"Los Angeles\") %&gt;% \n  distinct() %&gt;% #gets rid of repeats in the dataset\n  slice_max(n, n = 10) %&gt;%  #find the top 10 most popular cities if applicable\n  left_join(state_info, by = \"o_state_name\") %&gt;%  #left join with state_info dataset to get regions of each state\n  mutate(n_formatted = number(n, big.mark = \",\")) %&gt;% \n  #add coordinates to dataset to be able to map correctly\n  mutate(long = c(-118, -117, -122, -121, -88, -121, -112, -115, -122, -74),\n         lat = c(34, 33, 37, 38, 42, 37, 33, 36.3, 47, 40.5),\n         la_long = c(-118.001, -118, -118, -118, -118, -118, -118, -118, -118, -118),\n         la_lat = c(34, 34, 34, 34, 34, 34, 34, 34, 34, 34)\n  )\n\n#create new column that joins two columns and take out San Jose\ntop_ten_pooled &lt;- top_ten_pooled %&gt;% \n  unite(\"city_state\", o_cz_name, state.abb, sep = \"-\", remove = FALSE)%&gt;% \n  filter(city_state != \"San Jose-CA\")\n\n\n#view wrangled dataset: uncomment to view dataset\n#View(top_ten_pooled)"
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#visual-1-map",
    "href": "blog/2024-03-09-young-adult-migration/index.html#visual-1-map",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Visual 1, Map",
    "text": "Visual 1, Map\nThe first visual I will be creating is the map portion of the infographic. Because my question hinges on which cities have the most migration to Los Angeles, it’s important that the reader is able to understand what is going on when looking at it for the first 5 seconds. Below is the code and finish product I made to get the product I wanted:\n\n#create first color palette\ncol_pal_9 &lt;- c(\"black\", \"#0C3A0C\", \"#135913\", \"#187218\", \"#1D8C1D\", \"#20AC20\", \"#2ADB2A\", \"#48FE48\", \"#8AED8A\")\n\n#create second color palette\ncol_pal_8 &lt;- c(\"#0C3A0C\", \"#135913\", \"#187218\", \"#1D8C1D\", \"#20AC20\", \"#2ADB2A\", \"#8AED8A\", \"#9BD69B\")\n\nI started off by created the two color palettes that I’ll be using for the whole dataset. This lets me get the color palette finished and also that I will not have to repeat the same text of code over and over. After setting my color palette, I went ahead and started building my map.\n\nmap &lt;- top_ten_pooled %&gt;% #piping from top_ten_pooled\n  ggplot(aes(long, lat)) + #have aes as long (x) and lat (y)\n  borders(\"state\", col=\"grey30\", fill=\"gainsboro\") + #create map\n  geom_curve(aes(x=long, y=lat, xend=la_long, yend=la_lat, color = o_cz_name), #create ling segment\n             curvature = 0.1, \n             alpha = 0.8, \n             linewidth = 2, \n             color = col_pal_9) +\n  geom_point(color=ifelse(top_ten_pooled$city_state %in% c(\"Los Angeles-CA\"), \"#2d4c75\", \"black\"), \n             size = 3, \n             color = \"black\") + #create points and specify what color the LA point is verses for the others\n  coord_quickmap() #graph map\n\nAll this code above is to create the basis of the map and its relevant parts. Because I wanted to add labels onto the map to specify which city the points were representing, I used the function geom_label per city to help adjust where each label went. Because I did this function 9 times, I’ll just add one example of what I did to achieve this task:\n\n  #San Digeo------\ngeom_label(aes(label = ifelse(city_state %in% \n                                c(\"San Diego-CA\"), top_ten_pooled$o_cz_name, NA)),\n           fill = \"white\",\n           color = \"black\",\n           alpha = 1, \n           size = 5, \n           force = 3,\n           show.legend = FALSE,\n           point.padding = 3,\n           nudge_y = -1.1,\n           nudge_x = 1,\n           arrow = arrow(length = unit(1, \"cm\")),\n           max.overlaps = 20) \n\nAfter, I went ahead and changed up the theme to make it very minimal for my final output:\n\n  theme_void() + #have theme as void\n  #create theme to change layout of plot\n  theme(\n    legend.text = element_blank(),\n    legend.background = element_blank(),\n    legend.title = element_blank(),\n    legend.key = element_blank(),\n    legend.position = \"none\",\n    plot.background = element_rect(fill='transparent'),\n    plot.title = element_text(family = \"merri\", size = 25, color = \"#201B22\", hjust = 0),\n    plot.subtitle = element_text(family = \"merri\", face = \"italic\", size = 15, color = \"#201B22\", hjust = 0),\n    plot.margin = margin(r = 45, l = 45),\n    panel.border = element_blank(),\n    panel.background = element_blank()\n  )\n\n\nAnd here is my final product for visualization 1! A simple but easy to understand map of which cities people are moving from to go to Los Angeles"
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#visual-2-lollipop-graph",
    "href": "blog/2024-03-09-young-adult-migration/index.html#visual-2-lollipop-graph",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Visual 2: Lollipop Graph",
    "text": "Visual 2: Lollipop Graph\nI wanted the audience to understand how many people were coming from these cities to Los Angeles. To show this, I decided to create a lollipop graph to let the audience see which cities were more likely to move to Los Angeles.\n\n#Graph two: amount of people moving to Los Angeles\ntop_ten_lolli &lt;- top_ten_pooled %&gt;% #piping from top_ten_pooled\n  mutate(d_cz_name = forcats::fct_reorder(city_state, desc(n))) %&gt;% #rearrange order\n  filter(city_state != \"Los Angeles-CA\") %&gt;% #filter by taking out Los Angeles\n  ggplot(aes(x = city_state, #create the aes\n             y = n)) +\n  geom_segment(aes(x=d_cz_name, xend=d_cz_name, y=0, yend=n), #specify the length of each stick\n               size = 5, \n               alpha = 1,\n               color = col_pal_8) +\n  geom_point(size= 10, #specify where the points are located on the graph\n             color= \"#176417\", \n             fill=alpha(\"#8AD78A\", 0.3), alpha=0.7, shape=21, stroke=2) +\n  geom_text(aes(label = n_formatted), vjust = -1.8, family = \"merri\", size = 6) + #add text showing the amount per lollipop graph\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +\n  # Theme edits:\n  theme(axis.text.x = element_text(family = \"merri\", color = \"grey30\", size = 15, angle = 45, vjust = 1, hjust= 1), #change x axis font\n        axis.text.y = element_blank(), #take out y axis text\n        axis.title.y = element_blank(), #take out y axis title\n        panel.background = element_rect(fill='transparent', color = NA), #change background of graph to transparent\n        panel.grid = element_blank(), #take out grid\n        axis.title.x = element_blank(), #take out x axis title\n        axis.ticks = element_blank(), #take out tick marks\n        plot.margin = margin(l = 20), #extend the left margin out\n        plot.background = element_rect(fill = \"transparent\", color = NA) #make background transparent\n  ) \n\n\nHere, the code was not as complex as the code for the map. In this visualization, I used the theme() function to my advantage to get rid of any parts of the graph that I thought was unnecessary toward the final iteration of the infographic. The audience can also clearly see the amount of people that moved away per city."
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#visual-3-bar-graph",
    "href": "blog/2024-03-09-young-adult-migration/index.html#visual-3-bar-graph",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Visual 3: Bar Graph",
    "text": "Visual 3: Bar Graph\nMy third infographic I created was a bar graph comparing the young adults that were already living in Los Angeles to the total amount of people that moved towards Los Angeles. To fully create this visualization, I created two bar graphs, one to represent Los Angeles, and on to represent all other cities combined which is represented as a stacked bar plot. Below is the code I created to achieve this visual:\n\n#subset for only LA observation\nla_pooled &lt;- top_ten_pooled[1,]\n\n#subset for all other observations\nmigrate_pooled &lt;- top_ten_pooled[2:9,] %&gt;%\n  mutate(combined = \"Cities Combined\",\n         total_combined = \"Total Population of Cities\",\n         total = sum(top_ten_pooled$n))\n\n\n#create bar graph for LA population\nla_bar &lt;- ggplot(data = la_pooled, aes(x = city_state, y = n)) + #specify what datasets and aes I want to look at\n  geom_col(width = 0.5, fill = \"#1A4075\") + #specify width of bar\n  scale_y_continuous(limits = c(0, 1800000)) + #set y axis to be consistent with other graph\n  geom_text(aes(label = n_formatted), vjust = -0.7) + #create a text label to show total population of LA\n  #edit theme to preference\n  theme(\n    panel.background = element_blank(), #take out panel\n    axis.title.x = element_blank(), #take out x axis title\n    axis.title.y = element_blank(), #take out y axis title\n    plot.title.position = \"plot\", #reposition plot title\n    axis.text.y = element_blank(), #take out y axis text\n    axis.text.x = element_text(size = 13), #change x axis font size\n    axis.ticks.y = element_blank(), #take out y axis ticks\n    axis.ticks.x = element_blank(), #take out x axis ticks\n    plot.background = element_rect(fill='transparent', color = NA), #make transparent\n    panel.border = element_blank(), #take out panel\n    panel.grid = element_blank(), #take out grid\n    plot.margin = margin(r = 1) #shift right side margin\n  )\n\n\n#create bar graph on combined cities population\nbot_cities_bar_controlled_y &lt;- ggplot() +\n  geom_col(data = migrate_pooled, aes(fill = city_state, y = n, x = combined), width = 0.5, position = position_stack(vjust = 1), fill = col_pal_8) + #create columns to make them stack on top of each other\n  scale_y_continuous(limits = c(0, 1800000)) +  #set y axis to be consistent with other graph\n  guides(fill=guide_legend(title=\"Cities\")) + #set titles \n  annotate(\"text\", x = 1, y=160000, label = \"85,286\") + #add label to show total combined population\n  #edit theme as before\n  theme(\n    panel.background = element_blank(),\n    axis.title.y = element_blank(),\n    axis.title.x = element_blank(),\n    plot.background = element_rect(fill='transparent', color=NA),\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 13),\n    legend.title = element_text(size = 25),\n    legend.text = element_text(size = 20),\n    axis.ticks.y = element_blank(),\n    axis.ticks.x = element_blank(),\n    legend.position = \"none\",\n    panel.border = element_blank(),\n    panel.grid = element_blank()\n  )\n\n#combine the bar graphs with patchwork\ncombined_bar &lt;- (la_bar + bot_cities_bar_controlled_y) +\n  plot_annotation(theme = theme(plot.background = element_rect(fill='transparent', color=NA)))\n\n\nTo fully create this infographic, I used the package patchwork to stitch together these two bar graphs with each other to get an easily comparable visualization between the two bars. Something of interest to note here is that the combination of all cities combined is not even 1/10 of Los Angeles’s population that decided to stay there. It also shows the sheer size of the population of Los Angeles."
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#finished-product",
    "href": "blog/2024-03-09-young-adult-migration/index.html#finished-product",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Finished product",
    "text": "Finished product\n\nAnd here is our final product! After exporting the created visuals and using an online editing platform I was able to show my visualization here with additional explanations on what each visualization represents and make sure that the audience accurately understood the message each visualization was trying to convey. And I also hope that you were able to learn more on the process of how an infographic is created! A lot of time is spent trying to understand the data you are trying to visualize and coding each visualization also takes much time as well!"
  },
  {
    "objectID": "blog/2024-03-09-young-adult-migration/index.html#guidelines-i-followed",
    "href": "blog/2024-03-09-young-adult-migration/index.html#guidelines-i-followed",
    "title": "Visualizing the Migration Patterns of Young Adults in the United States",
    "section": "Guidelines I followed",
    "text": "Guidelines I followed\nWhen creating these sets of visualizations, I adhered to these guidelines that were given to me by my Data Visualization instructor to make sure I create a superb visualization. Here are all the guidelines that I followed:\n1. Graphic Form I decided to choose visualizations that would help the audience pick up on what type of message was being told. My first visualization is a map that shows the location of a specific city in question being connected to Los Angeles, the city that was of interest. This would give a visual sense of what types of connections I was making and let the reader know that I wanted to convey a message of moving from one city to the next.\n2. Text Most of the text relating to each plot were almost omitted. I created a couple of versions that included a plot with a title and a plot without the title. I decided to stick with the plots that did not have a title as it was easier to build my infographic as well as giving my infographic a clean, minimal look. The additions of plots with additional text made the infographic look clunky with having too much information posted per plot.\n3. Themes Compared to the theme that ggplot automatically render, I decided to change the theme completely by taking out almost parts that make up a graph to get just the graphic. This consisted of setting the theme to using both theme_void and by manually editing the theme function to my what I wanted my end graphic to look like. Once I was left with the graphic, I used that to build my infographic and added in informational text to explain each graphic.\n4. Colors Color theme was a tricky subject for this project as I didn’t know how to best convey the message as well as connect each plot with each other as they were all intertwined. One thought I had for the color scheme was to make each color related to the city it was tied with. However, I did not go through with that plan as some of the cities had overlapping colors and no color that defined that city. I ended up going with a color gradient to show that each datapoint is connected to each other by being a city as well as being able to easily connect each plot to the other graphs.\n5. Typography Since this was a travel related infographic, I wanted the text to be clean, but also inspire travel vibes. I decided to go with the font Merriweather as it has a newspaper like feel with it. To me, newspapers are associated with travel as it brings you news from all over the world right to your doorstep. The audience would feel like they are reading a newsletter they got in the mail and are then inspired to plan their next travel trip.\n6. General Design As I have been alluding to in the previous parts, my main goal was to make the infographic as clean and clear as possible. Because of this, I wanted to add visualizations that were easy to understand from a glance with minimal but necessary text. The first infographic that the reader would see is the map. I made sure that the map was big enough on the page so that the reader would be drawn to the map first and foremost. I put the lollipop graph and bar graph right next to each other as these are also informative graphs but are not as important as the map graph. The lollipop graph is put on the left side as most read from left to right, and I want the reader to look at the lollipop graph second, and the bar graph third. These are the main designs that I chose to implement for this infographic.\n7. Contextualizing your Data I wanted the reader to understand the data being shown on the infographic easily, so I did so by minimizing any unnecessary parts of the graph that the reader could get distracted with. The text bubbles are written to add context to the created graphic. For example, I tried to see how these plots would look like with a labeled y axis. But once I did that, it added clutter to the overall infographic which led me to created the text bubbles to compliment each graphic.\n8. Centering your Primary Message Since my topic is on migration patterns in America, I wanted to make sure that the reader could clearly follow the message that I wanted to convey to them. I wanted to make sure that my primary questions, “From what cities are young people moving to Los Angeles?” was answered in well and concise. Creating the map as a centerpiece became one of the main ways that I was able to center my primary message.\n9. Considering Accessibility Because of the red green colorblindness that exist in the population, I made sure that I did not add the color red in my infographic. Overall, my infographic only uses the colors green and blue, which are not in any color blindness combination. I also made sure to add alt text that was informative enough to explain each graphic of the infographic. I also made sure to make the alt text not too long as when screen readers are used in these situations, it cannot go back and re-read a sentence.\n10, Applying a DEI Lens to my Design The main people that are being looked at in this infographic are the Young People of America. I wanted to make sure that they were being accurately represented. I did so by looking at the cities that had the most amount of people that were moving to Los Angeles. From the data wrangling and data viewing that I did with my dataset, most of these cities that did move towards Los Angeles included a diverse population. This meant I could be more certain that I was fairly representing the young adult population well and not over representing a certain race"
  },
  {
    "objectID": "Blog_file.html",
    "href": "Blog_file.html",
    "title": "Blog",
    "section": "",
    "text": "Visualizing the Migration Patterns of Young Adults in the United States\n\n\n\n\n\n\n\nquarto\n\n\nvisualization\n\n\ninfographic\n\n\n\n\nA deeper dive looking into why young adults are choosing to live in Los Angeles\n\n\n\n\n\n\nMar 9, 2024\n\n\nPatty Park\n\n\n\n\n\n\n  \n\n\n\n\nLos Angeles County’s history with HOLC grade inequality\n\n\n\n\n\n\n\nR\n\n\nMEDS\n\n\nGeospatial\n\n\n\n\nA brief view of how HOLC grading propelled the inequality in Los Angeles County\n\n\n\n\n\n\nDec 15, 2023\n\n\nPatty Park\n\n\n\n\n\n\n  \n\n\n\n\nStatistically Analysing Nitrogen concentration in Vernal Pools\n\n\n\n\n\n\n\nR\n\n\nMEDS\n\n\nStatistics\n\n\n\n\nA statistical analysis on Nitrogen concentration levels in Vernal Pools in CCBER locations\n\n\n\n\n\n\nDec 15, 2023\n\n\nPatty Park\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of the Air Quality caused by the Thomas Fire\n\n\n\n\n\n\n\nPython\n\n\nMEDS\n\n\nData Visualization\n\n\n\n\nA personal analysis on air quality in Santa Barbara County and visualizing the burn area caused by the Thomas Fire\n\n\n\n\n\n\nDec 13, 2023\n\n\nPatty Park\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Patricia Park",
    "section": "",
    "text": "Welcome! I’m a recent Bren School graduate and data science enthusiast, skilled in geospatial analysis, machine learning, and data visualization. Explore my journey into the world of data-driven environmental solutions!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Patricia Park",
    "section": "Education",
    "text": "Education\n\nBS Environmental Studies at University of California, Santa Barbara (Grad. date: 2021)\nMEDS Environmental Data Science at University of California, Santa Barbara (Grad. date: 2024)"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Patricia Park",
    "section": "Projects",
    "text": "Projects\n\nDevelopment of an Interactive Visualization and Training Toolkit for Climate Impacts on the Channel Islands National Marine Sanctuary\nThis project focused on developing an educational toolkit and an interactive, Python-based web application to visualize ecologically significant climate variables within Channel Islands and Southern California.\nRepository | Project Information\n2024 MTA Open Data Challenge\nNew York’s public tranportation data was used to create a data visualization on the rider’s wait time for the bus. This was done for the New York’s MTA open data challenge, during October 2024.\nRepository"
  }
]